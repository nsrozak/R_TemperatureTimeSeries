---
title: "Time Series Analysis of Weather Data"
author: "Natalie Rozak"
date: "7/24/2020"
output: 
  github_document:
    pandoc_args: --webtex=https://ibm.codecogs.com/png.latex?
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

I used a dataset about weather in Seattle from 1948 to 2020 from the NOAA: https://www.ncdc.noaa.gov/cdo-web/datasets/GHCND/stations/GHCND:USW00024233/detail.

```{r global imports,warning=FALSE,message=FALSE}
# global imports
# data cleaning
library(tidyverse)
library(plyr)
library(MASS)
library(tseries)
# data visualization
library(kableExtra)
library(ggplot2)
library(gridExtra)
# modeling
library(forecast)

# surpress scientific notation
options(scipen=999)
```

# Preprocessing

```{r import data}
# import data
daily <- read.csv('~/Documents/GitHub/R_TemperatureTimeSeries/seattle_data.csv',
                  header=TRUE)
# remove last data point
daily <- daily %>% filter(DATE!='2020-07-10')
# select columns
daily <- daily %>% subset(select=c(DATE,TMAX))
```

```{r output data types}
# output structure
str(daily)
```

```{r convert data types}
# convert DATE to Date data type
daily$DATE <- as.Date(as.character(daily$DATE))
```

```{r missing values}
# output number of missing values in each column
kable(t(sapply(daily,function(x) sum(is.na(x))))) %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"), 
                full_width=TRUE,position='center')
```
```{r monthly data}
# create monthly data
monthly <- daily
# create month and year variables
monthly$month <- lubridate::month(monthly$DATE)
monthly$year <- lubridate::year(monthly$DATE)
# group by month and year
monthly <- monthly %>% dplyr::group_by(year,month) %>%
  dplyr::summarize(AVG_TMP=mean(TMAX)) 
# create date column
monthly <- monthly %>% mutate(DATE=lubridate::make_date(year,month,1))
# select columns
monthly <- monthly %>% subset(select=c(DATE,AVG_TMP))
```

```{r split into train and test}
# make 70% of the data train and 30% of the data test
train_index <- 1:(0.7*nrow(monthly))
train <- monthly[train_index,]
test <- monthly[-train_index,]
```

```{r create time series object}
# create time series object for train
train_ts <- ts(train$AVG_TMP,start=c(1948,1),frequency=12)
```

# Visualizations

***Time Series Plots***

```{r time series plot of original data,message=FALSE, fig.align='center',fig.width=10,fig.height=4}
# plot time series
ggplot() +
  geom_line(aes(x=train$DATE,y=train$AVG_TMP),color='deepskyblue2') +
  geom_line(aes(x=test$DATE,y=test$AVG_TMP),color='darkorange2') +
  labs(title='Time Series Plot of Temperature in Seattle') +
  ylab('Monthly Temperature (Fahrenheit)') +
  xlab('') +
  theme(plot.title=element_text(face='bold')) +
  stat_smooth(aes(x=train$DATE,y=train$AVG_TMP),color='maroon3',fill='maroon3',alpha=0.3)
```

***Decomposed Data***

```{r function for plotting time series}
# function for plotting time series
plot_time_series <- function(x,y,ylab) {
  ggplot() +
    geom_line(aes(x=x,y=y),color='deepskyblue2') +
    labs(title='Time Series Plot of Temperature in Seattle') +
    ylab(ylab) +
    xlab('') +
    theme(plot.title=element_text(face='bold')) +
    stat_smooth(aes(x=x,y=y),color='maroon3',fill='maroon3',alpha=0.3)
    
}
```

```{r decompose data,message=FALSE,warning=FALSE,fig.align='center',fig.width=10,fig.height=12}
# decompose the data
comp <- decompose(train_ts)
# create graphs
trend <- plot_time_series(x=train$DATE,y=comp$trend,ylab='Trend')
seasonal <- plot_time_series(x=train$DATE,y=comp$seasonal,ylab='Seasonal')
random <- plot_time_series(x=train$DATE,y=comp$random,ylab='Random')
# arrange graphs
grid.arrange(trend,seasonal,random,nrow=3)
```

The time series seems to have a slightly increasing trend and a seasonal effect with lag 12. 

***Autocorrelation and Partial Autocorrelation Plots***

The autocovariance function is $\gamma_x(t,s)=Cov(X_t,X_s)=E(X_tX_s)-E(X_t)E(X_s)$. The autocorrelation function is $\rho_x(t,s)=Corr(X_t,X_s)=\frac{Cov(X_t,X_s)}{\sqrt{Var(X_1)Var(X_s)}}=\frac{\gamma_x(t,s)}{\sigma_x(t)\sigma_x(s)}$. For stationary data, this function is simplified to $\rho_x(k)=\frac{\gamma_x(k)}{\gamma_x(0)}$. 

The partial autocorrelation function is $\alpha(0)=1$ and $\alpha(n)=\phi_{nn}$. The system of Yule-Walker equations $\phi_n=\mathbb{R}_n^{-1}\rho_n$ is used to find the partial autocorrelations: 
$\begin{bmatrix}1 & \rho_x(1) & \cdots & \rho_x(n-1) \\ \rho_x(1) & 1 & \cdots & \rho_x(n-2) \\ \vdots & \vdots & \ddots & \vdots & \\ \rho_x(1) & \rho_x(n-1) & \cdots & 1 \end{bmatrix} \begin{bmatrix} \phi_{n1}  \\ \phi_{n2}  \\ \vdots \\ \phi_{nn} \end{bmatrix} = \begin{bmatrix} \rho_x(1) \\ \rho_x(2) \\ \vdots \\ \rho_x(n) \end{bmatrix}$

The confidence intervals are estimated as $0\pm 1.96n^{-1/2}$.

```{r function for plotting acf and pacf}
# function for plottig acf/pacf
plot_acf_pacf <- function(lag,func,n,alpha,title,ylab){
  ggplot() +
    geom_segment(aes(x=lag,y=0,xend=lag,yend=func),col='deepskyblue2') +
    geom_point(aes(x=lag,y=func),col='deepskyblue2') +
    geom_hline(yintercept=c(-1,1)*qnorm((1+alpha)/2)/sqrt(n),
               lty=2,col='darkorange2') +
    labs(title=title) +
    ylab(ylab) +
    xlab('Lag') +
    theme(plot.title=element_text(face='bold'))
}
```

```{r acf graph of original data,fig.align='center',fig.width=6,fig.height=4}
# create acf data frame
train_acf <- acf(train_ts,lag.max=50,plot=FALSE)
# create graph
plot_acf_pacf(train_acf$lag,train_acf$acf,train_acf$n.used,
              0.95,'Autocorrelation Plot','ACF')
```

```{r pacf graph of original data,fig.align='center',fig.width=6,fig.height=4}
# create acf data frame
train_pacf <- pacf(train_ts,lag.max=50,plot=FALSE)
# create graph
plot_acf_pacf(train_pacf$lag,train_pacf$acf,train_pacf$n.used,
              0.95,'Partial Autocorrelation Plot','PACF')
```

# Stationary Data

Stationary data has a constant variance, no trend, and no seasonality.

## Stabilize Variance

Box-cox transformations are used to stabilize the variance. They are defined as:
$y_i^{\lambda} \begin{cases} ln(y_i) &\text{if } \lambda=0,y_i\geq0 \\ \frac{y^{\lambda}-1}{\lambda} &\text{if } \lambda\neq0,y_i\geq0 \end{cases}$. The confidence interval for $\hat\lambda$ is $\ell(\lambda)\geq\ell(\hat\lambda)-\frac{1}{2}\chi^2_{1,1-\alpha}$.

```{r boxcox,fig.align='center',fig.width=6,fig.height=4}
# find lambda
t <- 1:nrow(train)
lambdas <- boxcox(train_ts~t,plotit=FALSE)
# obtain best lambda
best_lambda <- lambdas$x[which(lambdas$y==max(lambdas$y))]
# create confidence interval
ci_lambda <- lambdas$x[lambdas$y > max(lambdas$y)-qchisq(0.95,1)/2]
lambdas_df <- data.frame(lambdas$x,lambdas$y) %>% 
  plyr::rename(c('lambdas.x'='x','lambdas.y'='y'))
# plot lambdas
ggplot() +
  geom_line(data=lambdas_df,aes(x=x,y=y),color='deepskyblue2') +
  geom_vline(xintercept=best_lambda,linetype='dashed',color='darkorange2')+
  geom_ribbon(data=subset(lambdas_df,x>=min(ci_lambda)&x<=max(ci_lambda)),
              aes(x=x,ymin=-1181,ymax=y),
            fill='deepskyblue2',alpha=0.3) +
  labs(title='Boxcox Transformation') +
  ylab('Log Likelihood') +
  xlab('Lambda') +
  theme(plot.title=element_text(face='bold'))
```

``` {r boxcox values}
# output best lambda
cat('Best lambda: ',best_lambda,'\n')
# output confidence interval
cat('Lower bound of confidence interval: ',min(ci_lambda),'\n')
cat('Upper bound of confidence interval: ',max(ci_lambda),'\n')
```

Since the confidence interval for $\hat\lambda$ does not contain 1, a transformation is needed to stabilize the variance. I transformed the data by using $\lambda=0.5$ which takes the square root of the temperature values.

```{r boxcox transform data}
# transform the data
train_ts_bc = sqrt(train_ts)
```


## Test for Trend

The Dickey-Fuller test is used to determine if the data is non-stationary due to a trend; the Kwiatkowski-Phillips-Schmidt-Shin test is used to determine if the data is non-stationary without differencing.

```{r stationary tests, warning=FALSE}
# adf test
adf <- adf.test(train_ts_bc,k=0)
if (adf$p.value<=0.05){
  cat('Reject null hypothesis for DF test: data is stationary.\n')
}else{
  cat('Fail to reject null hypothesis for DF test: data is not stationary\n')
}
# kpss test
kpss <- kpss.test(train_ts_bc)
if(kpss$p.value<=0.05){
  cat('Reject null hypothesis for KPSS test: data is not stationary.\n')
}else{
  cat('Fail to reject null hypothesis for KPSS test: data is stationary\n')
}
```

Since the data is stationary in both cases, there is no need to correct for trend. When looking at the decomposed graph for trend, we see the data does not have a smooth trend, so not differencing for a trend seems reasonable.

## Remove Seasonality

I differenced once at lag 12 to remove seasonality: $Y_t=\nabla_{12}X_t = (1-B^{12})X_t = X_t - X_{t-12}$.

```{r time series plot of final data,message=FALSE,fig.align='center',fig.width=6,fig.height=4}
# data differenced at lag 1
diff_seas <- diff(train_ts_bc,lag=12,differences=1)
# adjust date
removed <- 1:(length(train_ts_bc)-length(diff_seas))
diff_seas_date <- train$DATE[-removed]
# create graph
plot_time_series(diff_seas_date,diff_seas,
                 '12th Difference of the Square Rooted Monthly Temperature')
```

The time series plot for the final data appears stationary, looking like white noise.

```{r acf graph of trend differenced data,fig.align='center',fig.width=6,fig.height=4}
# create acf data frame
diff_seas_acf <- acf(diff_seas,lag.max=50,plot=FALSE)
# create graph
plot_acf_pacf(diff_seas_acf$lag,diff_seas_acf$acf,diff_seas_acf$n.used,
              0.95,'Autocorrelation Plot','ACF')
```

```{r pacf graph of trend differenced data,fig.align='center',fig.width=6,fig.height=4}
# create acf data frame
diff_seas_pacf <- pacf(diff_seas,lag.max=85,plot=FALSE)
# create graph
plot_acf_pacf(diff_seas_pacf$lag,diff_seas_pacf$acf,diff_seas_pacf$n.used,
              0.95,'Partial Autocorrelation Plot','PACF')
```

# SARIMA Model

***Find Parameters***

Based off of the ACF and PACF plots, I suspect that the model parameters are close to SARIMA$(0,0,1)\times(0,1,1)_{12}$. To choose the best model, I used grid search to find the parameters that minimize Akaike Information Criterion (AIC). The formula for AIC is $AIC=-2ln(L(\theta_q,\phi_p,\Theta_Q,\Phi_P,\sigma_z^2))+2k$ where $L(\theta_q,\phi_p,\Theta_Q,\Phi_P,\sigma_z^2)$ are the  maximum likelihood estimators and $k$ is the number of estimated parameters. 

```{r xreg param}
# output mean
cat('Mean of differenced time series: ',mean(diff_seas))
```

Since the mean is very close to 0, I will not include a constant term in the model.

```
# create vectors to try
p_vec <- c(0:3,11)
q_vec <- c(0:3)
P_vec <- c(0:6)
Q_vec <- c(0:1)
# find best AICC
AIC <- NULL
for (p in p_vec){
  for (q in q_vec){
    for (P in P_vec){
      for (Q in Q_vec){
        model <- try(Arima(train_ts_bc,
                       order=c(p,0,q),
                       seasonal=list(order=c(P,1,Q),period=12),
                       method='CSS-ML',
                       optim.method='L-BFGS-B',
                       optim.control=list(maxit=500)),TRUE)
        if (isTRUE(class(model)=='try-error')){
          next
        }else{
          AIC <- rbind(AIC,c(p,q,P,Q,model$aic))
        }
      }
    }
  }
}
# rename columns
colnames(AIC) <- c('p','q','P','Q','aic')
# convert to data frame
AIC_df <- as.data.frame(AIC)
# output best AIC
head(AIC_df[order(AIC_df$aic),])
```

***Find Coefficients***

```{r initial model}
# create final model
initial_model <- Arima(train_ts_bc,order=c(3,0,0),seasonal=list(order=c(2,1,1),period=12),
                       method='CSS-ML',optim.method='L-BFGS-B',optim.control=list(maxit=500))
```

```{r initial model coeffcients}
# variance matrix for coefficients
initial_var_matrix <- vcov(initial_model)
# ar1
cat('The AR1 coefficient is: ',initial_model$coef[1],
    '.\n The lower bound for the confidence interval is: ',
    initial_model$coef[1]-1.96*sqrt(initial_var_matrix[1,1]),
    '.\n The upper bound for the confidence interval is: ',
    initial_model$coef[1]+1.96*sqrt(initial_var_matrix[1,1]),'.\n')
# ar2
cat('The AR2 coefficient is: ',initial_model$coef[2],
    '.\n The lower bound for the confidence interval is: ',
    initial_model$coef[2]-1.96*sqrt(initial_var_matrix[2,2]),
    '.\n The upper bound for the confidence interval is: ',
    initial_model$coef[2]+1.96*sqrt(initial_var_matrix[2,2]),'.\n')
# ar3
cat('The AR3 coefficient is: ',initial_model$coef[3],
    '.\n The lower bound for the confidence interval is: ',
    initial_model$coef[3]-1.96*sqrt(initial_var_matrix[3,3]),
    '.\n The upper bound for the confidence interval is: ',
    initial_model$coef[3]+1.96*sqrt(initial_var_matrix[3,3]),'.\n')
# sar1
cat('The SAR1 coefficient is: ',initial_model$coef[4],
    '.\n The lower bound for the confidence interval is: ',
    initial_model$coef[4]-1.96*sqrt(initial_var_matrix[4,4]),
    '.\n The upper bound for the confidence interval is: ',
    initial_model$coef[4]+1.96*sqrt(initial_var_matrix[4,4]),'.\n')
# sar2
cat('The SAR2 coefficient is: ',initial_model$coef[5],
    '.\n The lower bound for the confidence interval is: ',
    initial_model$coef[5]-1.96*sqrt(initial_var_matrix[5,5]),
    '.\n The upper bound for the confidence interval is: ',
    initial_model$coef[5]+1.96*sqrt(initial_var_matrix[5,5]),'.\n')
# sma1
cat('The SMA1 coefficient is: ',initial_model$coef[6],
    '.\n The lower bound for the confidence interval is: ',
    initial_model$coef[6]-1.96*sqrt(initial_var_matrix[6,6]),
    '.\n The upper bound for the confidence interval is: ',
    initial_model$coef[6]+1.96*sqrt(initial_var_matrix[6,6]),'.\n')
```

The confidence interval for the AR2 and SAR1 coefficients contain 0 so I will remove these terms from the final model. 

***Final Model***

```{r final model}
# create final model
sarima_model <- Arima(train_ts_bc,order=c(3,0,0),seasonal=list(order=c(2,1,1),period=12),
                      fixed=c(NA,0,NA,0,NA,NA),
                      method='CSS-ML',optim.method='L-BFGS-B',optim.control=list(maxit=500))
```

```{r final model coefficients}
# variance matrix for coefficients
var_matrix <- vcov(sarima_model)
# ar1
cat('The AR1 coefficient is: ',sarima_model$coef[1],
    '.\n The lower bound for the confidence interval is: ',
    sarima_model$coef[1]-1.96*sqrt(var_matrix[1,1]),
    '.\n The upper bound for the confidence interval is: ',
    sarima_model$coef[1]+1.96*sqrt(var_matrix[1,1]),'.\n')
# ar3
cat('The AR3 coefficient is: ',sarima_model$coef[3],
    '.\n The lower bound for the confidence interval is: ',
    sarima_model$coef[3]-1.96*sqrt(var_matrix[2,2]),
    '.\n The upper bound for the confidence interval is: ',
    sarima_model$coef[3]+1.96*sqrt(var_matrix[2,2]),'.\n')
# sar2
cat('The SAR2 coefficient is: ',sarima_model$coef[5],
    '.\n The lower bound for the confidence interval is: ',
    sarima_model$coef[5]-1.96*sqrt(var_matrix[3,3]),
    '.\n The upper bound for the confidence interval is: ',
    sarima_model$coef[5]+1.96*sqrt(var_matrix[3,3]),'.\n')
# sma1
cat('The SMA1 coefficient is: ',sarima_model$coef[6],
    '.\n The lower bound for the confidence interval is: ',
    sarima_model$coef[6]-1.96*sqrt(var_matrix[4,4]),
    '.\n The upper bound for the confidence interval is: ',
    sarima_model$coef[6]+1.96*sqrt(var_matrix[4,4]),'.\n')
```

The final model is SARIMA$(3,0,0)\times(2,1,1)_{12}$ with $\phi(B)\Phi(B^{12})(1-B^{12})X_t=\Theta(B^{12})Z_t$ with $\phi(B)=1-0.221B-0.14B^3$, $\Phi(B^{12})=1+0.11B^{24}$, and $\Theta(B^{12})=1-0.892B^{12}$. The solved equation is $X_t=0.221X_{t-1}+0.14X_{t-3}+X_{t-12}-0.221X_{t-13}-0.14X_{t-15}-0.11X_{t-24}+0.024X_{t-25}+0.015X_{t-27}+0.11X_{t-36}-0.024X_{t-37}-0.015X_{t-39}+Z_t-0.823Z_{t-12}$.

```{r output final model AIC}
# output the AIC for the final model
cat('The AIC for the final model is: ',sarima_model$aic,'\n')
```

# Diagnostic Checking

## Residuals are White Noise

Residuals are $\hat{W}_t = \frac{X_t-\hat{X}_t}{\sqrt{r_t-1}}$.

```{r obtain residuals}
# residuals
residuals <- residuals(sarima_model,standardized=FALSE)
# standardized residuals
standardized_residuals <- residuals(sarima_model,standardized=TRUE)
```

***Time Series Plot***

```{r plot residuals,message=FALSE,fig.align='center',fig.width=6,fig.height=4}
# plot standardized residuals
plot_time_series(train$DATE,standardized_residuals,'Standardized Residuals')
```

The time series plot of the residuals resembles white noise.

***ACF and PACF Plots***

```{r residual acf plot,fig.align='center',fig.width=6,fig.height=4}
# create acf data frame
resid_acf <- acf(residuals,lag.max=20,plot=FALSE)
# create graph
plot_acf_pacf(resid_acf$lag,resid_acf$acf,resid_acf$n.used,
              0.95,'Autocorrelation Plot','ACF')
```

```{r residual pacf plot,fig.align='center',fig.width=6,fig.height=4}
# create acf data frame
resid_pacf <- pacf(residuals,lag.max=20,plot=FALSE)
# create graph
plot_acf_pacf(resid_pacf$lag,resid_pacf$acf,resid_pacf$n.used,
              0.95,'Partial Autocorrelation Plot','PACF')
```

Residuals are uncorrelated based on the ACF and PACF plots.

***Portmanteau Statistics***

Null hypothesis: residuals are uncorrelated. Alternative hypothesis: residuals are correlated.

The Box-Pierce test for linear dependence uses test statistic $Q_w=n\sum_{j=1}^h\hat{\rho}(j)^2$ where $Q_w\sim\chi_{h-p-q}^2$ and $h\approx\sqrt{n}$. The Ljung test for linear dependence uses test statistic $\tilde{Q}_w=n(n+2)\sum_{j=1}^h\frac{\hat{\rho}(j)^2}{n-j}$ where $\tilde{Q}_w\sim\chi_{h-p-q}^2$ and $h\approx\sqrt{n}$. The Mcleod Li test for noninear dependence uses test statistic $\tilde{Q}_{WW}=n(n+2)\sum_{j=1}^h\frac{\hat{\rho}_{\hat{W}\hat{W}}(j)^2}{n-j}$ where $\tilde{Q}_{WW}\sim\chi_h^2$, $\hat{\rho}_{\hat{W}\hat{W}}(h)=\frac{\sum_{t=1}^{n-h}(\hat{W}_t^2-\bar{W}^2)(\hat{W}_{t+h}^2-\bar{W}^2)}{\sum_{t=1}^n(\hat{W}_t^2-\bar{W}^2)^2}$, $\bar{W}^2=\frac{1}{n}\sum_{t=1}^n\hat{W}_t^2$, and $h\approx\sqrt{n}$. 

```{r portmanteau statistics}
h <- sqrt(length(train_ts_bc))
# Box-Pierce test
box_pierce <- Box.test(residuals,lag=h,type=c('Box-Pierce'),fitdf=7)
if (box_pierce$p.value<=0.05){
  cat('Reject null hypothesis for Box-Pierce test: residuals are correlated\n')
}else{
  cat('Fail to reject null hypothesis for Box-Pierce test: residuals are uncorrelated\n')
}
# Ljung test
ljung <- Box.test(residuals,lag=h,type=c('Ljung-Box'),fitdf=7)
if (ljung$p.value<=0.05){
  cat('Reject null hypothesis for Ljung test: residuals are correlated\n')
}else{
  cat('Fail to reject null hypothesis for Ljung test: residuals are uncorrelated\n')
}
# Mcleod Li test
mcleod_li <- Box.test(residuals^2,lag=h,type=c('Ljung-Box'),fitdf=0)
if (mcleod_li$p.value<=0.05){
  cat('Reject null hypothesis for Mcleod Li test: residuals are correlated\n')
}else{
  cat('Fail to reject null hypothesis for Mcleod Li test: residuals are uncorrelated\n')
}
```

## Residuals are Normally Distributed

***Q-Q Plot***

```{r qq plot,message=FALSE,fig.align='center',fig.width=6,fig.height=4}
# create qq plot
ggplot(data.frame(y=standardized_residuals),aes(sample=y)) +
  stat_qq_line(color='darkorange2') + stat_qq(color='deepskyblue2') +
  labs(title='Q-Q Plot of Standardized Residuals') +
  theme(plot.title=element_text(face='bold'))
  
```

The Q-Q plot is linear, indicating that residuals are normally distributed.

***Shapiro-Wilk Test***

Null hypothesis: residuals follow a normal distribution. Alternative hypothesis: residuals are not from a normal distribution.

```{r shapiro wilk test}
# shapiro wilk test
shapiro <- shapiro.test(residuals)
if (shapiro$p.value<=0.05){
  cat('Reject null hypothesis for Shapiro-Wilk test: residuals are not normal\n')
}else{
  cat('Fail to reject null hypothesis for Shapiro-Wilk test: residuals are normal\n')
}
```

# Forecasting

```{r predictions,warning=FALSE}
# obtain predictions
predictions <- predict(sarima_model,n.ahead=nrow(test))
# obtain confidence interval
pred_ci_lower <- predictions$pred - 1.96*predictions$se
pred_ci_upper <- predictions$pred + 1.96*predictions$se
```

```{r convert predictions back to original units}
# convert predictions back to original values
orig_pred <- predictions$pred^2
orig_pred_ci_lower <- pred_ci_lower^2
orig_pred_ci_upper <- pred_ci_upper^2
```

***Plot Predictions***

```{r time series plot of forecasts,message=FALSE,  fig.align='center',fig.width=10,fig.height=4}
# plot time series
ggplot() +
  geom_line(aes(x=train$DATE,y=train$AVG_TMP),color='deepskyblue2') +
  geom_line(aes(x=test$DATE,y=orig_pred),color='darkorange2') +
  geom_ribbon(aes(x=test$DATE,ymin=orig_pred_ci_lower,ymax=orig_pred_ci_upper),
              fill='darkorange2',alpha=0.3) +
  labs(title='Time Series Plot of Temperature in Seattle with Forecasts') +
  ylab('Monthly Temperature (Fahrenheit)') +
  xlab('') +
  theme(plot.title=element_text(face='bold'))
```

```{r time series plot of just forecasts,message=FALSE,  fig.align='center',fig.width=10,fig.height=4}
# plot time series
ggplot() +
  geom_line(aes(x=test$DATE,y=orig_pred),color='darkorange2') +
  geom_ribbon(aes(x=test$DATE,ymin=orig_pred_ci_lower,ymax=orig_pred_ci_upper),
              fill='darkorange2',alpha=0.3) +
  labs(title='Time Series Plot of Forecasted Temperature in Seattle') +
  ylab('Monthly Temperature (Fahrenheit)') +
  xlab('') +
  theme(plot.title=element_text(face='bold'))
```

***Root Mean Squared Error***

```{r forecast evaluation functions}
# rmse function
rmse <- function(y,y_hat){
  return(sqrt(mean((y-y_hat)^2)))
}
```
```{r rmse of test data}
# root mean squared error
cat('RMSE on the test data: ',rmse(test$AVG_TMP,orig_pred),'\n')
```