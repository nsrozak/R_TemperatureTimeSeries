diff_seas <- diff(train_ts_bc,lag=12,differences=1)
# adjust date
removed <- 1:(length(train_ts_bc)-length(diff_seas))
diff_seas_date <- train$DATE[-removed]
# create graph
plot_time_series(diff_seas_date,diff_seas,
'12th Difference of the Square Rooted Temperature')
# create acf data frame
diff_seas_acf <- acf(diff_seas,lag.max=50,plot=FALSE)
# create graph
plot_acf_pacf(diff_seas_acf$lag,diff_seas_acf$acf,diff_seas_acf$n.used,
0.95,'Autocorrelation Plot','ACF')
# create acf data frame
diff_seas_pacf <- pacf(diff_seas,lag.max=85,plot=FALSE)
# create graph
plot_acf_pacf(diff_seas_pacf$lag,diff_seas_pacf$acf,diff_seas_pacf$n.used,
0.95,'Partial Autocorrelation Plot','PACF')
# create data frame
data_ml <- monthly
# create sqrt variable
data_ml$SQRT_AVG_TMP <- sqrt(data_ml$AVG_TMP)
# create differenced variable
data_ml$y <- data_ml$SQRT_AVG_TMP-dplyr::lag(data_ml$SQRT_AVG_TMP,n=12)
# remove extra rows from data_ml
data_ml <- data_ml[-(1:12),]
# create lagged variables
data_ml$x1 <- dplyr::lag(data_ml$y,n=1)
data_ml$x2 <- dplyr::lag(data_ml$y,n=2)
data_ml$x3 <- dplyr::lag(data_ml$y,n=3)
data_ml$x11 <- dplyr::lag(data_ml$y,n=11)
data_ml$x12 <- dplyr::lag(data_ml$y,n=12)
# create indices for machine learning
train_index_ml <- 1:612
# split data into train and test
train_ml <- data_ml[train_index_ml,]
test_ml <- data_ml[-train_index_ml,]
# remove extra rows from train_ml
train_ml <- train_ml[-(1:12),]
# select columns
train_df <- train_ml %>% subset(select=c(y,x1,x2,x3,x11,x12))
test_df <- test_ml %>% subset(select=c(y,x1,x2,x3,x11,x12))
# rmse function
rmse <- function(y,y_hat){
return(sqrt(mean((y-y_hat)^2)))
}
# initialize matrix for holding rmse results
rmse_results <- NULL
# create model
holt_winters <- ets(train_ts,model='ANA',lambda=0.5,opt.crit='mse',ic='aic',damped=FALSE)
# obtain predictions
es_fitted <- window(holt_winters$fitted,start=c(1950,1))
es_year_forecasts <- forecast(holt_winters,h=nrow(test_ml[1:12,]))
es_year_pred <- es_year_forecasts$mean
es_forecasts <- forecast(holt_winters,h=nrow(test_ml))
es_predictions <- es_forecasts$mean
# add results to matrix
rmse_results <- rbind(rmse_results, c('Exponential Smoothing',
rmse(train_ml$AVG_TMP,es_fitted),
rmse(test_ml[1:12,]$AVG_TMP,es_year_pred),
rmse(test_ml$AVG_TMP,es_predictions)))
# output parameters
cat('The alpha parameter is ',holt_winters$par[1],
'\nThe gamma parameter is ',holt_winters$par[2],'\n')
# output AIC
cat('The AIC of the final model is ',holt_winters$aic,'\n')
# obtain residuals
es_residuals <- window(holt_winters$residuals,start=c(1950,1))
# plot residuals
plot_time_series(train_ml$DATE,es_residuals,'Residuals')
# create acf data frame
es_resid_acf <- acf(es_residuals,lag.max=20,plot=FALSE)
# create graph
plot_acf_pacf(es_resid_acf$lag,es_resid_acf$acf,es_resid_acf$n.used,
0.95,'Autocorrelation Plot','ACF')
# create acf data frame
es_resid_pacf <- pacf(es_residuals,lag.max=20,plot=FALSE)
# create graph
plot_acf_pacf(es_resid_pacf$lag,es_resid_pacf$acf,es_resid_pacf$n.used,
0.95,'Partial Autocorrelation Plot','PACF')
# function for creating qq plot
qq_plot <- function(residuals){
ggplot(data.frame(y=residuals),aes(sample=y)) +
stat_qq_line(color='orange') + stat_qq(color='deepskyblue2') +
labs(title='Q-Q Plot of Standardized Residuals') +
theme(plot.title=element_text(face='bold'))
}
# qq plot
qq_plot(es_residuals)
# shapiro-wilk test
es_shapiro <- shapiro.test(es_residuals)
hypothesis_test(es_shapiro$p.value,'Shapiro-Wilk',
'residuals are not normal','residuals are normal')
# output mean
cat('Mean of differenced time series: ',mean(diff_seas))
# create final model
initial_model <- Arima(train_ts_bc,order=c(3,0,0),seasonal=list(order=c(2,1,1),period=12),
method='CSS-ML',optim.method='L-BFGS-B',optim.control=list(maxit=500))
# function for outputting coefficient confidence interval
coefficient_ci <- function(term,coefficient,error){
cat('The ',term,' estimated coefficient is ',coefficient,
'\nI am 95% confident that the true coefficient is between',
coefficient-error,' and ',coefficient+error,'\n')
}
# variance matrix for coefficients
initial_var_matrix <- vcov(initial_model)
# AR1
coefficient_ci('AR1',initial_model$coef[1],1.96*sqrt(initial_var_matrix[1,1]))
# AR2
coefficient_ci('AR2',initial_model$coef[2],1.96*sqrt(initial_var_matrix[2,2]))
# AR3
coefficient_ci('AR3',initial_model$coef[3],1.96*sqrt(initial_var_matrix[3,3]))
# SAR1
coefficient_ci('SAR1',initial_model$coef[4],1.96*sqrt(initial_var_matrix[4,4]))
# SAR2
coefficient_ci('SAR2',initial_model$coef[5],1.96*sqrt(initial_var_matrix[5,5]))
# SMA1
coefficient_ci('SMA1',initial_model$coef[6],1.96*sqrt(initial_var_matrix[6,6]))
# create final model
sarima_model <- Arima(train_ts_bc,order=c(3,0,0),seasonal=list(order=c(2,1,1),period=12),
fixed=c(NA,0,NA,0,NA,NA),
method='CSS-ML',optim.method='L-BFGS-B',optim.control=list(maxit=500))
# obtain predictions
sarima_fitted <- window(sarima_model$fitted,start=c(1950,1))^2
sarima_year_forecasts <- forecast(sarima_model,h=nrow(test_ml[1:12,]))
sarima_year_pred <- sarima_year_forecasts$mean^2
sarima_forecasts <- forecast(sarima_model,h=nrow(test_ml))
sarima_predictions <- sarima_forecasts$mean^2
# add results to matrix
rmse_results <- rbind(rmse_results, c('SARIMA',
rmse(train_ml$AVG_TMP,sarima_fitted),
rmse(test_ml[1:12,]$AVG_TMP,sarima_year_pred),
rmse(test_ml$AVG_TMP,sarima_predictions)))
# variance matrix for coefficients
var_matrix <- vcov(sarima_model)
# AR1
coefficient_ci('AR1',initial_model$coef[1],1.96*sqrt(initial_var_matrix[1,1]))
# AR3
coefficient_ci('AR3',initial_model$coef[3],1.96*sqrt(initial_var_matrix[2,2]))
# SAR2
coefficient_ci('SAR2',initial_model$coef[5],1.96*sqrt(initial_var_matrix[3,3]))
# SMA1
coefficient_ci('SMA1',initial_model$coef[6],1.96*sqrt(initial_var_matrix[4,4]))
# output the AIC for the final model
cat('The AIC for the final model is: ',sarima_model$aic,'\n')
# residuals
sarima_residuals <- window(residuals(sarima_model,standardized=FALSE),start=c(1950,1))
# standardized residuals
sarima_standardized_residuals <-
window(residuals(sarima_model,standardized=TRUE),start=c(1950,1))
# plot residuals
plot_time_series(train_ml$DATE,sarima_standardized_residuals,'Residuals')
# create acf data frame
sarima_resid_acf <- acf(sarima_residuals,lag.max=20,plot=FALSE)
# create graph
plot_acf_pacf(sarima_resid_acf$lag,sarima_resid_acf$acf,sarima_resid_acf$n.used,
0.95,'Autocorrelation Plot','ACF')
# create acf data frame
sarima_resid_pacf <- pacf(sarima_residuals,lag.max=20,plot=FALSE)
# create graph
plot_acf_pacf(sarima_resid_pacf$lag,sarima_resid_pacf$acf,sarima_resid_pacf$n.used,
0.95,'Partial Autocorrelation Plot','PACF')
h <- sqrt(length(train_ts_bc))
# Box-Pierce test
box_pierce <- Box.test(sarima_residuals,lag=h,type=c('Box-Pierce'),fitdf=)
hypothesis_test(box_pierce$p.value,'Box-Pierce',
'residuals are correlated','residuals are uncorrelated')
# Ljung test
ljung <- Box.test(sarima_residuals,lag=h,type=c('Ljung-Box'),fitdf=)
hypothesis_test(ljung$p.value,'Box-Pierce',
'residuals are correlated','residuals are uncorrelated')
# Mcleod Li test
mcleod_li <- Box.test(sarima_residuals^2,lag=h,type=c('Ljung-Box'),fitdf=0)
hypothesis_test(mcleod_li$p.value,'Box-Pierce',
'residuals are correlated','residuals are uncorrelated')
# qq plot
qq_plot(sarima_standardized_residuals)
# shapiro-wilk test
sarima_shapiro <- shapiro.test(sarima_residuals)
hypothesis_test(sarima_shapiro$p.value,'Shapiro-Wilk',
'residuals are not normal','residuals are normal')
# make folds
folds <- createTimeSlices(1:nrow(train_df),initialWindow=180,horizon=36,fixedWindow=FALSE,
skip=35)
train_folds <- folds[[1]]
test_folds <- folds[[2]]
# function for converting data back to original units
original_units <- function(nabla_Y_lst,Y_12_lst){
return((nabla_Y_lst+Y_12_lst)^2)
}
# create time series objects for ml testing datasets
# note that the values start 1 year ahead so they can be matched with lag 12 observations
train_ml_ts <- ts(train_ml$x12,start=c(1949,1),frequency=12)
test_ml_year_ts <- ts(test_ml[1:12,]$x12,start=c(1999,1),frequency=12)
test_ml_ts <- ts(test_ml$x12,start=c(1999,1),frequency=12)
# create Y_12_lst for all test data frames
Y_12_train <- ts.intersect(train_ml_ts,data_ts_bc)
Y_12_train <- as.numeric(Y_12_train[,2])
Y_12_test_year <- ts.intersect(test_ml_year_ts,data_ts_bc)
Y_12_test_year <- as.numeric(Y_12_test_year[,2])
Y_12_test <- ts.intersect(test_ml_ts,data_ts_bc)
Y_12_test <- as.numeric(Y_12_test[,2])
# function for cross validation
knn_cv <- function(neighbor){
rmse_lst <- NULL
for(i in 1:length(train_folds)){
knn_i <- knnreg(x=subset(train_df[train_folds[[i]],],select=-c(y)),
y=train_df[train_folds[[i]],]$y,k=neighbor)
pred_i <- predict(knn_i,newdata=subset(train_df[test_folds[[i]],],select=-c(y)))
rmse_lst <- c(rmse_lst,rmse(train_df[test_folds[[i]],]$y,pred_i))
}
return(mean(rmse_lst))
}
# store rmse
knn_rmse <- NULL
# tree vectors
neighbors <- c(2,5,7,10,20,50)
# cross validation
for (neighbor in neighbors){
knn_rmse <- rbind(knn_rmse, c(neighbor,knn_cv(neighbor)))
}
# rename columns
colnames(knn_rmse) <- c('Number of Neighbors','RMSE')
# output random forest cv results
kable(knn_rmse) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=TRUE,position='center')
# knn model
knn <- knnreg(x=subset(train_df,select=-c(y)),y=train_df$y,k=20)
# make differenced predictions
diff_knn_fitted <- predict(knn,newdata=subset(train_df,select=-c(y)))
diff_knn_year_pred <- predict(knn,newdata=subset(test_df[1:12,],select=-c(y)))
diff_knn_predictions <- predict(knn,newdata=subset(test_df,select=-c(y)))
# convert back to original units
knn_fitted <- original_units(diff_knn_fitted,Y_12_train)
knn_year_pred <- original_units(diff_knn_year_pred,Y_12_test_year)
knn_predictions <- original_units(diff_knn_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('K Nearest Neighbors',
rmse(train_ml$AVG_TMP,knn_fitted),
rmse(test_ml[1:12,]$AVG_TMP,knn_year_pred),
rmse(test_ml$AVG_TMP,knn_predictions)))
# function for cross validation
rf_cv <- function(tree){
rmse_lst <- NULL
for(i in 1:length(train_folds)){
set.seed(137)
rf_i <-  randomForest(y~.,data=train_df[train_folds[[i]],],
ntree=tree,mtry=ceiling(5/3))
pred_i <- predict(rf_i,newdata=train_df[test_folds[[i]],])
rmse_lst <- c(rmse_lst,rmse(train_df[test_folds[[i]],]$y,pred_i))
}
return(mean(rmse_lst))
}
# store rmse
rf_rmse <- NULL
# tree vectors
trees <- c(50,100,300,500,700)
# cross validation
for (tree in trees){
rf_rmse <- rbind(rf_rmse, c(tree,rf_cv(tree)))
}
# rename columns
colnames(rf_rmse) <- c('Number of Trees','RMSE')
# output random forest cv results
kable(rf_rmse) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=TRUE,position='center')
# create random forest model
set.seed(137)
rf <- randomForest(y~.,data=train_df,ntree=700,mtry=ceiling(5/3))
# make differenced predictions
diff_rf_fitted <- predict(rf,newdata=train_df)
diff_rf_year_pred <- predict(rf,newdata=test_df[1:12,])
diff_rf_predictions <- predict(rf,newdata=test_df)
# convert back to original units
rf_fitted <- original_units(diff_rf_fitted,Y_12_train)
rf_year_pred <- original_units(diff_rf_year_pred,Y_12_test_year)
rf_predictions <- original_units(diff_rf_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('Random Forest',
rmse(train_ml$AVG_TMP,rf_fitted),
rmse(test_ml[1:12,]$AVG_TMP,rf_year_pred),
rmse(test_ml$AVG_TMP,rf_predictions)))
# function for cross validation
gbt_cv <- function(tree,depth,shrink){
rmse_lst <- NULL
for(i in 1:length(train_folds)){
set.seed(347)
gbt_i <- gbm(y~.,data=train_df[train_folds[[i]],],distribution='gaussian',
n.trees=tree,interaction.depth=depth,shrinkage=shrink)
pred_i <- predict(gbt_i,newdata=train_df[test_folds[[i]],])
rmse_lst <- c(rmse_lst,rmse(train_df[test_folds[[i]],]$y,pred_i))
}
return(mean(rmse_lst))
}
# store rmse
gbt_rmse <- NULL
# parameter vectors
depths <- c(1,2,3,5,10)
shrinks <- c(0.01,0.1,0.5,0.9)
# cross validation
for (tree in trees){
for (depth in depths){
for (shrink in shrinks){
gbt_rmse <- rbind(gbt_rmse, c(tree,depth,shrink,gbt_cv(tree,depth,shrink)))
}
}
}
# rename columns
colnames(gbt_rmse) <- c('Number of Trees','Depth','Shrinkage','RMSE')
# convert to data frame
gbt_rmse_df <- as.data.frame(gbt_rmse)
# output gbt cv results
kable(head(gbt_rmse_df[order(gbt_rmse_df$RMSE),])) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=TRUE,position='center')
# create gbt model
set.seed(347)
gbt <- gbm(y~.,data=train_df,distribution='gaussian',
n.trees=500,interaction.depth=1,shrinkage=0.01)
# make differenced predictions
diff_gbt_fitted <- predict(gbt,newdata=train_df)
diff_gbt_year_pred <- predict(gbt,newdata=test_df[1:12,])
diff_gbt_predictions <- predict(gbt,newdata=test_df)
# convert back to original units
gbt_fitted <- original_units(diff_gbt_fitted,Y_12_train)
gbt_year_pred <- original_units(diff_gbt_year_pred,Y_12_test_year)
gbt_predictions <- original_units(diff_gbt_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('Gradient Boosted Tree',
rmse(train_ml$AVG_TMP,gbt_fitted),
rmse(test_ml[1:12,]$AVG_TMP,gbt_year_pred),
rmse(test_ml$AVG_TMP,gbt_predictions)))
# create linear regression model
ols <- lm(y~.,data=train_df)
# make differenced predictions
diff_ols_fitted <- predict(ols,newdata=train_df)
diff_ols_year_pred <- predict(ols,newdata=test_df[1:12,])
diff_ols_predictions <- predict(ols,newdata=test_df)
# convert back to original units
ols_fitted <- original_units(diff_ols_fitted,Y_12_train)
ols_year_pred <- original_units(diff_ols_year_pred,Y_12_test_year)
ols_predictions <- original_units(diff_ols_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('Ordinary Least Squares',
rmse(train_ml$AVG_TMP,ols_fitted),
rmse(test_ml[1:12,]$AVG_TMP,ols_year_pred),
rmse(test_ml$AVG_TMP,ols_predictions)))
# function for cross validation
glmnet_cv <- function(lambda,alpha){
rmse_lst <- NULL
for(i in 1:length(train_folds)){
glmnet_i <-  glmnet(x=model.matrix(y~., data=train_df[train_folds[[i]],])[,-1],
y=train_df[train_folds[[i]],]$y,
family='gaussian',lambda=lambda,alpha=alpha)
pred_i <- predict(glmnet_i,newx=model.matrix(y~., data=train_df[test_folds[[i]],])[,-1])
rmse_lst <- c(rmse_lst,rmse(train_df[test_folds[[i]],]$y,pred_i))
}
return(mean(rmse_lst))
}
# store rmse
ridge_rmse <- NULL
# lambda vectors
lambdas <- c(0.0001,0.001,0.01,0.1,1,10)
# cross validation
for (lambda in lambdas){
ridge_rmse <- rbind(ridge_rmse, c(lambda,glmnet_cv(lambda,0)))
}
# rename columns
colnames(ridge_rmse) <- c('Lambda','RMSE')
# output ridge cv results
kable(ridge_rmse) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=TRUE,position='center')
# create linear regression model
ridge <- glmnet(x=model.matrix(y~., data=train_df)[,-1],y=train_df$y,
family='gaussian',lambda=0.001,alpha=0)
# make differenced predictions
diff_ridge_fitted <- predict(ridge,newx=model.matrix(y~., data=train_df)[,-1])
diff_ridge_year_pred <- predict(ridge,newx=model.matrix(y~., data=test_df[1:12,])[,-1])
diff_ridge_predictions <- predict(ridge,newx=model.matrix(y~., data=test_df)[,-1])
# convert back to original units
ridge_fitted <- original_units(diff_ridge_fitted,Y_12_train)
ridge_year_pred <- original_units(diff_ridge_year_pred,Y_12_test_year)
ridge_predictions <- original_units(diff_ridge_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('Ridge',
rmse(train_ml$AVG_TMP,ridge_fitted),
rmse(test_ml[1:12,]$AVG_TMP,ridge_year_pred),
rmse(test_ml$AVG_TMP,ridge_predictions)))
# store rmse
lasso_rmse <- NULL
# cross validation
for (lambda in lambdas){
lasso_rmse <- rbind(lasso_rmse, c(lambda,glmnet_cv(lambda,1)))
}
# rename columns
colnames(lasso_rmse) <- c('Lambda','RMSE')
# output lasso cv results
kable(lasso_rmse) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=TRUE,position='center')
# create linear regression model
lasso <- glmnet(x=model.matrix(y~., data=train_df)[,-1],y=train_df$y,
family='gaussian',lambda=0.01,alpha=1)
# make differenced predictions
diff_lasso_fitted <- predict(lasso,newx=model.matrix(y~., data=train_df)[,-1])
diff_lasso_year_pred <- predict(lasso,newx=model.matrix(y~., data=test_df[1:12,])[,-1])
diff_lasso_predictions <- predict(lasso,newx=model.matrix(y~., data=test_df)[,-1])
# convert back to original units
lasso_fitted <- original_units(diff_lasso_fitted,Y_12_train)
lasso_year_pred <- original_units(diff_lasso_year_pred,Y_12_test_year)
lasso_predictions <- original_units(diff_lasso_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('Lasso',
rmse(train_ml$AVG_TMP,lasso_fitted),
rmse(test_ml[1:12,]$AVG_TMP,lasso_year_pred),
rmse(test_ml$AVG_TMP,lasso_predictions)))
# create linear regression model
svm_model <-  svm(x=model.matrix(y~., data=train_df)[,-1],y=train_df$y,
kernel='polynomial',cost=0.1,epsilon=0.5,gamma=0.0001,coef0=0.5,degree=3)
# make differenced predictions
diff_svm_fitted <- predict(svm_model,newdata=model.matrix(y~., data=train_df)[,-1])
diff_svm_year_pred <- predict(svm_model,newdata=model.matrix(y~., data=test_df[1:12,])[,-1])
diff_svm_predictions <- predict(svm_model,newdata=model.matrix(y~., data=test_df)[,-1])
# convert back to original units
svm_fitted <- original_units(diff_svm_fitted,Y_12_train)
svm_year_pred <- original_units(diff_svm_year_pred,Y_12_test_year)
svm_predictions <- original_units(diff_svm_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('Support Vector Machine',
rmse(train_ml$AVG_TMP,svm_fitted),
rmse(test_ml[1:12,]$AVG_TMP,svm_year_pred),
rmse(test_ml$AVG_TMP,svm_predictions)))
# function for cross validation
ann_cv <- function(node){
rmse_lst <- NULL
for(i in 1:length(train_folds)){
set.seed(728)
ann_i <-  neuralnet(y~.,data=train_df[train_folds[[i]],],hidden=node)
pred_i <- predict(ann_i,newdata=train_df[test_folds[[i]],])
rmse_lst <- c(rmse_lst,rmse(train_df[test_folds[[i]],]$y,pred_i))
}
return(mean(rmse_lst))
}
# store rmse
ann_rmse <- NULL
# tree vectors
nodes <- c(1,3,5,6,8)
# cross validation
for (node in nodes){
ann_rmse <- rbind(ann_rmse, c(node,ann_cv(node)))
}
# rename columns
colnames(ann_rmse) <- c('Number of Nodes','RMSE')
# output random forest cv results
kable(ann_rmse) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=TRUE,position='center')
# create random forest model
set.seed(728)
ann <- neuralnet(y~.,data=train_df,hidden=1)
# make differenced predictions
diff_ann_fitted <- predict(ann,newdata=train_df)
diff_ann_year_pred <- predict(ann,newdata=test_df[1:12,])
diff_ann_predictions <- predict(ann,newdata=test_df)
# convert back to original units
ann_fitted <- original_units(diff_ann_fitted,Y_12_train)
ann_year_pred <- original_units(diff_ann_year_pred,Y_12_test_year)
ann_predictions <- original_units(diff_ann_predictions,Y_12_test)
# add results to matrix
rmse_results <- rbind(rmse_results, c('Neural Network',
rmse(train_ml$AVG_TMP,ann_fitted),
rmse(test_ml[1:12,]$AVG_TMP,ann_year_pred),
rmse(test_ml$AVG_TMP,ann_predictions)))
colnames(rmse_results) <- c('Model', 'Train Data', 'First Test Year', 'All Test Data')
# output rmse from all models
kable(rmse_results) %>%
kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),
full_width=TRUE,position='center')
# function for plotting predictions
plot_predictions <- function(predictions, ci_lower, ci_upper){
ggplot() +
geom_line(aes(x=test_ml$DATE,y=test_ml$AVG_TMP),color='orange') +
geom_line(aes(x=test_ml$DATE,y=predictions),color='maroon3',linetype='dotted',size=1) +
geom_ribbon(aes(x=test_ml$DATE,ymin=ci_lower,ymax=ci_upper),
fill='maroon3',alpha=0.2) +
labs(title='Time Series Plot of Forecasted Temperature in Seattle') +
ylab('Monthly Temperature (Fahrenheit)') +
xlab('') +
theme(plot.title=element_text(face='bold'))
}
# obtain confidence interval
es_ci_lower <- es_forecasts$lower[,2]
es_ci_upper <- es_forecasts$upper[,2]
# plot exponential smoothing time series
ts_plot +
geom_line(aes(x=test_ml$DATE,y=es_predictions),color='maroon3',linetype='dotted') +
geom_ribbon(aes(x=test_ml$DATE,ymin=es_ci_lower,ymax=es_ci_upper),
fill='maroon3',alpha=0.2)
# plot exponential smoothing predictions
plot_predictions(es_predictions,es_ci_lower,es_ci_upper)
# obtain confidence interval
sarima_ci_lower <- sarima_forecasts$lower[,2]^2
sarima_ci_upper <- sarima_forecasts$upper[,2]^2
# plot sarima time series
ts_plot +
geom_line(aes(x=test_ml$DATE,y=sarima_predictions),color='maroon3',linetype='dotted') +
geom_ribbon(aes(x=test_ml$DATE,ymin=sarima_ci_lower,ymax=sarima_ci_upper),
fill='maroon3',alpha=0.2)
# plot sarima predictions
plot_predictions(sarima_predictions,sarima_ci_lower,sarima_ci_upper)
